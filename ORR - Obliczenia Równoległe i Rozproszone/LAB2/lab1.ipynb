{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cMGzTMN8R68"
      },
      "source": [
        "<div align=\"center\"><h1>Accelerating Applications with CUDA C/C++</h1></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgZVeIdE8R6_"
      },
      "source": [
        "Aby przyspieszyć wykonywanie programu, należy przenieść obliczenia z CPU. Wiele przełomów powstałych dzięki akceleracji obliczeniowej, stale rosnące zapotrzebowanie na akcelerowane aplikacje, konwencje programowania ułatwiające ich pisanie oraz ciągłe ulepszanie sprzętu, który je obsługuje, napędzają tę nieuniknioną zmianę.\n",
        "\n",
        "W centrum sukcesu przyspieszonego przetwarzania, zarówno pod względem imponującej wydajności, jak i łatwości użytkowania, znajduje się platforma obliczeniowa CUDA. CUDA zapewnia paradygmat kodowania, który rozszerza języki takie jak C, C++, Python i Fortran, aby umożliwić uruchamianie akcelerowanego, masowo zrównoleglonego kodu na najwydajniejszych na świecie procesorach równoległych: procesorach graficznych NVIDIA. CUDA drastycznie przyspiesza aplikacje przy niewielkim wysiłku, ma ekosystem wysoce zoptymalizowanych bibliotek dla DNN, BLAS, analizy wykresów, FFT i innych, a także jest dostarczany z potężnymi liniami poleceń i profilami wizualnymi.\n",
        "\n",
        "CUDA obsługuje wiele, jeśli nie większość, najbardziej wydajnych aplikacji na świecie w: obliczeniowej dynamice płynów, dynamice molekularnej, chemii kwantowej, fizyce i HPC.\n",
        "\n",
        "Nauka CUDA pozwoli na przyspieszenie własnych aplikacji. Przyspieszone aplikacje działają znacznie szybciej niż ich odpowiedniki wykorzystujące tylko procesory i umożliwiają obliczenia, które w innym przypadku byłyby niemożliwe do wykonania ze względu na ograniczoną wydajność aplikacji wykorzystujących tylko procesory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opFJZ-EP8R7A"
      },
      "source": [
        "---\n",
        "## Cele\n",
        "\n",
        "Po dzisiejszych zajęciach powinniście być w stanie:\n",
        "\n",
        "- Pisać, kompilować i uruchamiać programy w języku C/C++, które zarówno wywołują funkcje na CPU oraz **uruchamiają kernele** na GPU.\n",
        "- Kontrolować równoległą **hierarchię wątków** wykorzystując **konfigurację wykonania**.\n",
        "- Przerabiać pętle, aby iteracje wykonywały się równolegle.\n",
        "- Alokować i zwalniać pamięć dostepną zarówno dla procesorów jak i kart graficznych.\n",
        "- Obsługiwać błędy generowane przez kod CUDA.\n",
        "- Przyspieszać aplikacje wykorzystujące tylko CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRGYE8VQ8R7A"
      },
      "source": [
        "---\n",
        "## Accelerated Systems\n",
        "\n",
        "*Systemy akcelerowane*, zwane również systemami *heterogenicznymi* to te, które składają się zarówno z procesorów CPU, jak i GPU. Systemy te uruchamiają programy CPU, które z kolei uruchamiają funkcje wykorzystujące obliczenia równoległe zapewniane przez GPU. Zajęcia opierają się o taki system, który zawiera procesor graficzny NVIDIA. Informacje o tym GPU można uzyskać za pomoca polecenia wiersza poleceń `nvidia-smi` (*Systems Management Inferface*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3aVBzvl8R7B",
        "outputId": "8309f33b-379a-43ed-a18b-ea4b0adbce53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 21 15:54:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tpS3Rls8R7B"
      },
      "source": [
        "---\n",
        "## GPU-accelerated Vs. CPU-only Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK4l-33-8R7B"
      },
      "source": [
        "---\n",
        "## Pisanie kodu aplikacji dla GPU\n",
        "\n",
        "CUDA dostarcza rozszerzenia dla wielu popularnych języków programowania, w przypadku tych zajęć C/C++. Te rozszerzenia językowe umożliwiają programistom uruchamianie funkcji w ich kodzie źródłowym na GPU.\n",
        "\n",
        "Poniżej znajduje się plik `.cu` (`.cu` to rozszerzenie pliku dla programów z akceleracją CUDA). Zawiera dwie funkcje, pierwszą działającą na CPU, drugą działającą na GPU. Można zauważyć różnice między funkcjami, zarówno pod względem ich definicji, jak i sposobu ich wywoływania.\n",
        "\n",
        "```cpp\n",
        "void CPUFunction()\n",
        "{\n",
        "  printf(\"This function is defined to run on the CPU.\\n\");\n",
        "}\n",
        "\n",
        "__global__ void GPUFunction()\n",
        "{\n",
        "  printf(\"This function is defined to run on the GPU.\\n\");\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  CPUFunction();\n",
        "\n",
        "  GPUFunction<<<1, 1>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "}\n",
        "```\n",
        "\n",
        "Oto kilka ważnych linijek kodu do podkreślenia, a także kilka innych pojęć używanych w akceleracji obliczeniowej:\n",
        "\n",
        "`__global__ void GPUFunction()`\n",
        "- Słowo kluczowe `__global__` wskazuje, że następująca funkcja będzie działać na GPU i może zostać wywołana **globalnie**, co w tym kontekście oznacza albo przez CPU, albo przez GPU.\n",
        "- Często kod wykonywany na procesorze jest określany jako kod **hosta** (*host*), a kod działający na GPU jest określany jako kod **urządzenia** (*device*).\n",
        "\n",
        "`GPUFunction<<<1, 1>>>();`\n",
        "- Zwykle, gdy wywołujemy funkcję uruchamianą na GPU, nazywamy tę funkcję **kernelem**, który jest **uruchamiany**.\n",
        "- Podczas uruchamiania kernela musimy zapewnić **konfigurację wykonania**, która jest wykonywana przy użyciu składni `<<< ... >>>` tuż przed przekazaniem kernelowi wszelkich oczekiwanych argumentów.\n",
        "- Na wysokim poziomie konfiguracja wykonania pozwala programistom określić **hierarchię wątków** do uruchomienia kernela, która definiuje liczbę grupowań wątków (zwanych **blokami**), a także liczbę **wątków** do wykonania w każdym bloku. Konfiguracja wykonywania zostanie szczegółowo omówiona później, na ten moment należy zauważyć, że kernel uruchamia się z jednym blokiem wątków `1` (pierwszy argument konfiguracji wykonania), który zawiera jeden wątek `1` (drugi argument konfiguracyjny).\n",
        "\n",
        "`cudaDeviceSynchronize();`\n",
        "- W przeciwieństwie do większości kodu C/C++, uruchamianie kernela jest **asynchroniczne**: kod procesora będzie nadal wykonywany *bez oczekiwania na zakończenie uruchamiania kernela*.\n",
        "- Wywołanie `cudaDeviceSynchronize`, funkcji dostarczanej przez środowisko wykonawcze CUDA, spowoduje, że kod hosta (CPU) będzie czekał na zakończenie kodu urządzenia (GPU), a dopiero potem wznowi wykonywanie na procesorze."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFs3dhor8R7C"
      },
      "source": [
        "---\n",
        "### Ćwiczenie: Write a Hello GPU Kernel\n",
        "\n",
        "Plik `01-hello-gpu.cu` zawiera program, który już działa. Zawiera dwie funkcje, obie z komunikatami \"Hello from the CPU\". Celem jest przerobienie funkcji `helloGPU` w pliku źródłowym, tak aby faktycznie działała na GPU i wyświetlała komunikat wskazujący, że działa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZmUfmK88R7C",
        "outputId": "38b9c22e-f4f4-46d9-f72d-07a3e8437a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from the CPU.\n",
            "Hello also from the CPU.\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_70 -o hello-gpu 01-hello-gpu.cu -run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIb_FHJB8R7C"
      },
      "source": [
        "Po udanej refaktoryzacji `01-hello-gpu.cu` dokonamy następujących modyfikacji, próbując skompilować i uruchomić po każdej zmianie. Otrzymamy błędy, należy poświęcić trochę czasu na ich uważne przeczytanie: ich znajomość będzie bardzo przydatna, w pisaniu własnego przyspieszonego kodu.\n",
        "- Usuń słowo kluczowe `__global__` z definicji kernela. Zwróć uwagę na numer wiersza w błędzie: jak myślisz, co oznacza w błędzie „skonfigurowany”? Po zakończeniu przywróć `__global__`.\n",
        "- Usuń konfigurację wykonywania: czy twoje rozumienie „skonfigurowanego” nadal ma sens? Po zakończeniu zamień konfigurację wykonania.\n",
        "- Usuń wywołanie `cudaDeviceSynchronize`. Przed skompilowaniem i uruchomieniem kodu zgadnij, co się stanie, pamiętając, że kernele są uruchamiane asynchronicznie, a `cudaDeviceSynchronize` powoduje, że wykonanie hosta czeka na zakończenie wykonywania jądra przed kontynuowaniem. Po zakończeniu przywróć `cudaDeviceSynchronize`.\n",
        "- Zmodyfikuj `01-hello-gpu.cu`, aby `Hello from the GPU` było wypisane **przed** Hello z CPU.\n",
        "- Zmodyfikuj `01-hello-gpu.cu`, aby `Hello from the GPU` było wypisane **dwa razy**, raz **przed** `Hello from the CPU` i raz **po**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKcV6cKL8R7C"
      },
      "source": [
        "---\n",
        "### Kompilacja i uruchamianie przyspieszonego kodu CUDA\n",
        "\n",
        "Ta sekcja zawiera szczegółowe informacje o poleceniu `nvcc`, którego użyto powyżej, aby skompilować i uruchomić program `.cu`.\n",
        "\n",
        "Platforma CUDA zawiera [**NVIDIA CUDA Compiler**](http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html) `nvcc`, który służy do kompilacji aplikacji z akceleracją CUDA, zarówno hosta jak i urządzenia. Zostaną omówione niezbędne elementy, w celu uzyskania szczegółowych informacji o `nvcc` należy zapoznać się z [dokumentacją](http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html)\n",
        "\n",
        "`nvcc` jest bardzo podobne do polecenia `gcc`. Na przykładz kopilacja jakiegoś pliku `przyklad-CUDA.cu` mogłaby wyglądać nastepująco:\n",
        "`nvcc -arch=sm_70 -o out przyklad-CUDA.cu -run`\n",
        "  - `nvcc` to polecenie wiersza poleceń do używania kompilatora `nvcc`.\n",
        "  - `przyklad-CUDA.cu` jest przekazywany jako plik do skompilowania.\n",
        "  - Flaga `o` jest używana do określenia pliku wyjściowego dla skompilowanego programu.\n",
        "  - Flaga `arch` wskazuje, dla której **architektury** pliki muszą być skompilowane. Aby dopasować tą flagę do własnego GPU, należy zapoznać się z [flagą `arch`](http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-steering-gpu-code-generation), [virtual architecture features](http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list) i [GPU features](http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list).\n",
        "  - Podanie flagi `run` spowoduje wykonanie pomyślnie skompilowanego pliku binarnego."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1dzY7ZK8R7C"
      },
      "source": [
        "---\n",
        "## CUDA Thread Hierarchy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKZPKNbA8R7C"
      },
      "source": [
        "---\n",
        "## Launching Parallel Kernels\n",
        "\n",
        "Konfiguracja wykonania pozwala programistom określić szczegóły dotyczące uruchamiania kernela do równoległego działania na wielu **wątkach** GPU. Dokładniej, konfiguracja wykonania pozwala programistom określić, ile grup wątków - zwanych **blokami wątków** lub po prostu **blokami** - i ile wątków ma zawierać każdy blok wątków. Składnia tego jest następująca:\n",
        "\n",
        "`<<< NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK>>>`\n",
        "\n",
        "** Kod kernela jest wykonywany przez każdy wątek w bloku, skonfigurowanym podczas uruchamiania kernela**.\n",
        "\n",
        "Zatem przy założeniu, że zdefiniowano kernel o nazwie `someKernel`, można powiedzieć że:\n",
        "   - `someKernel<<<1, 1>>>()` jest skonfigurowany do działania w jednym bloku, który ma jeden wątek i dlatego będzie działał tylko raz.\n",
        "   - `someKernel<<<1, 10>>>()` jest skonfigurowany do działania w jednym bloku, który ma 10 wątków i dlatego będzie działał 10 razy.\n",
        "   - `someKernel<<<10, 1>>>()` jest skonfigurowany do działania w 10 blokach wątków, z których każdy ma jeden wątek i dlatego będzie działał 10 razy.\n",
        "   - `someKernel<<<10, 10>>>()` jest skonfigurowany do działania w 10 blokach wątków, z których każdy ma 10 wątków i dlatego będzie działał 100 razy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpmODiQu8R7C"
      },
      "source": [
        "---\n",
        "### Ćwiczenie: Launch Parallel Kernels\n",
        "\n",
        "Plik `01-first-parallel.cu` obecnie wykonuje bardzo proste wywołanie funkcji, która wyświetla komunikat `This should be running in parallel.` Wykonaj poniższe czynności, aby najpierw dokonać refaktoryzacji w celu uruchomienia na GPU, a następnie równolegle, w jednym, a następnie w wielu blokach wątków:\n",
        "\n",
        "- Refaktoryzacja funkcji `firstParallel`, aby uruchomić ją jako kernel CUDA na GPU. Po skompilowaniu i uruchomieniu `01-first-parallel.cu` za pomocą poniższego polecenia `nvcc` nadal powinno być widoczne wyjście funkcji.\n",
        "- Refaktoryzuj kernel `firstParallel`, aby było wykonywane równolegle w 5 wątkach, wszystkie wykonywane w jednym bloku wątku. Powinieneś zobaczyć komunikat wyjściowy wydrukowany 5 razy po skompilowaniu i uruchomieniu kodu.\n",
        "- Ponownie zrefaktoryzuj kernel `firstParallel`, tym razem tak, aby było wykonywane równolegle wewnątrz 5 bloków wątków, z których każdy zawiera 5 wątków. Powinieneś zobaczyć komunikat wyjściowy wydrukowany 25 razy po skompilowaniu i uruchomieniu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om5P80la8R7C"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_70 -o first-parallel 01-first-parallel.cu -run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkcAfuwt8R7D"
      },
      "source": [
        "---\n",
        "\n",
        "## CUDA-Provided Thread Hierarchy Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1dCBV8k8R7D"
      },
      "source": [
        "---\n",
        "## Indeksy wątków i bloków\n",
        "\n",
        "Każdy wątek otrzymuje indeks w swoim bloku wątków, zaczynając od „0”. Dodatkowo każdy blok otrzymuje indeks, zaczynając od „0”. Podobnie jak wątki są pogrupowane w bloki, bloki są pogrupowane w **grid**, który jest najwyższą jednostką w hierarchii wątków CUDA. Podsumowując, kernele CUDA są wykonywane w gridzie z 1 lub więcej bloków, przy czym każdy blok zawiera taką samą liczbę  wątków.\n",
        "\n",
        "Kernele CUDA mają dostęp do specjalnych zmiennych identyfikujących zarówno indeks wątku (w bloku), który wykonuje kernel, jak i indeks bloku (w gridzie), w którym znajduje się wątek. Te zmienne to odpowiednio `threadIdx.x` i `blockIdx.x`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvFH7SoL8R7D"
      },
      "source": [
        "---\n",
        "### Ćwiczenie: Używanie określonych indeksów wątków i bloków\n",
        "\n",
        "Obecnie plik `01-thread-and-block-idx.cu` zawiera działający kernel, który wyświetla komunikat o błędzie. Otwórz plik, aby dowiedzieć się, jak zaktualizować konfigurację wykonywania, aby wyświetlić komunikat o powodzeniu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bmp5HqRv8R7D",
        "outputId": "f4de9c87-1d85-4a94-b1a7-9ce0a55435a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_70 -o thread-and-block-idx 01-thread-and-block-idx.cu -run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byyoHmaX8R7D"
      },
      "source": [
        "---\n",
        "## Przyspieszanie pętli for\n",
        "\n",
        "Pętle for w aplikacjach wykorzystujących tylko procesor są łatwe do przyspieszenia: zamiast uruchamiać każdą iterację pętli szeregowo, każda iteracja może być uruchamiana równolegle we własnym wątku. Rozważ następującą pętlę for i zauważ, że kontroluje ona, ile razy pętla zostanie wykonana, a także definiuje, co stanie się w każdej iteracji pętli:\n",
        "\n",
        "```cpp\n",
        "int N = 2<<20;\n",
        "for (int i = 0; i < N; ++i)\n",
        "{\n",
        "  printf(\"%d\\n\", i);\n",
        "}\n",
        "```\n",
        "\n",
        "Aby zrównoleglić tę pętlę, należy wykonać 2 kroki:\n",
        "\n",
        "- Kernel musi być napisany, aby wykonać pracę **pojedynczej iteracji pętli**.\n",
        "- Ponieważ kernel będzie niezależny od innych działających kerneli, konfiguracja wykonania musi być taka, aby kernel wykonał się odpowiednią liczbę razy, na przykład, ile razy wykonałaby iterację pętli."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc2M4evl8R7D"
      },
      "source": [
        "---\n",
        "### Ćwiczenie: Przyspieszenie pętli For z pojedynczym blokiem wątków\n",
        "\n",
        "Obecnie funkcja `loop` wewnątrz `01-single-block-loop.cu` uruchamia pętlę for, która będzie wypisywać szeregowo liczby od „0” do „9”. Refaktoryzuj funkcję `loop` tak, aby była kernelem CUDA, który uruchomi się, aby wykonywać równolegle `N` iteracji. Po pomyślnej refaktoryzacji liczby od „0” do „9” nadal powinny być drukowane."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQuw6uEF8R7D"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_70 -o single-block-loop 01-single-block-loop.cu -run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjNmVAIk8R7D"
      },
      "source": [
        "---\n",
        "## Koordynowanie równoległych wątków"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUh8WB-G8R7D"
      },
      "source": [
        "---\n",
        "## Wykorzystywanie wymiarów bloków do większej równoległości\n",
        "\n",
        "Istnieje ograniczenie liczby wątków, które mogą istnieć w bloku wątków: dokładnie 1024. Aby zwiększyć ilość równoległości w akcelerowanych aplikacjach, musimy być w stanie koordynować między wieloma blokami wątków.\n",
        "\n",
        "Kernele CUDA mają dostęp do specjalnej zmiennej, która podaje liczbę wątków w bloku: `blockDim.x`. Używając tej zmiennej, w połączeniu z `blockIdx.x` i `threadIdx.x`, można uzyskać zwiększoną równoległość, organizując równoległe wykonywanie wielu bloków wielu wątków za pomocą wyrażenia `threadIdx.x + blockIdx.x * blockDim.x `. Przykład:\n",
        "\n",
        "Konfiguracja wykonania `<<<10, 10>>>` uruchomiłaby siatkę zawierającą w sumie 100 wątków, zawartą w 10 blokach po 10 wątków. Mamy zatem nadzieję, że każdy wątek będzie mógł obliczyć pewien unikalny dla siebie indeks między „0” a „99”.\n",
        "\n",
        "- Jeśli blok `blockIdx.x` równa się `0`, to `blockIdx.x * blockDim.x` to `0`. Dodając do `0` możliwe wartości `threadIdx.x` od `0` do `9`, możemy wygenerować indeksy od `0` do `9` w obrębie siatki 100 wątków.\n",
        "- Jeśli blok `blockIdx.x` równa się `1`, to `blockIdx.x * blockDim.x` to `10`. Dodając do `10` możliwe wartości `threadIdx.x` o wartościach od `0` do `9`, możemy wygenerować indeksy od `10` do `19` w obrębie siatki 100 wątków.\n",
        "- Jeśli blok `blockIdx.x` równa się `5`, to `blockIdx.x * blockDim.x` to `50`. Dodając do `50` możliwe wartości `threadIdx.x` od `0` do `9`, możemy wygenerować indeksy od `50` do `59` w obrębie siatki 100 wątków.\n",
        "- Jeśli blok `blockIdx.x` równa się `9`, to `blockIdx.x * blockDim.x` to `90`. Dodając do `90` możliwe wartości `threadIdx.x` o wartościach od `0` do `9`, możemy wygenerować indeksy od `90` do `99` w obrębie siatki 100 wątków."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3aDh8CU8R7D"
      },
      "source": [
        "---\n",
        "### Ćwiczenie: Przyspieszenie pętli for z wieloma blokami wątków\n",
        "\n",
        "Obecnie funkcja `loop` wewnątrz `02-multi-block-loop.cu` uruchamia pętlę for, która seryjnie wyświetla numery od `0` do `9`. Refaktoryzuj funkcję `loop` tak, aby była kernelem CUDA, który uruchomi się, aby wykonywać równolegle `N` iteracji. Po pomyślnej refaktoryzacji liczby od `0` do `9` nadal powinny być wyświetlane. W tym ćwiczeniu jako dodatkowe ograniczenie użyj konfiguracji wykonania, która uruchamia *co najmniej 2 bloki wątków.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01M955Sa8R7D"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_70 -o multi-block-loop 02-multi-block-loop.cu -run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIVufWTF8R7D"
      },
      "source": [
        "---\n",
        "## Przydzielanie pamięci, która będzie dostępna na GPU i CPU\n",
        "\n",
        "Nowsze wersje CUDA (wersja 6 i nowsze) ułatwiły przydzielanie pamięci, która jest dostępna zarówno dla hosta procesora, jak i dowolnej liczby urządzeń GPU, i chociaż istnieje wiele [technik średniozaawansowanych i zaawansowanych](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations) do zarządzania pamięcią, która będzie obsługiwać najbardziej optymalną wydajność w akcelerowanych aplikacjach, najbardziej podstawowa technika zarządzania pamięcią CUDA, którą teraz omówimy zapewnia duże przyrosty wydajności w porównaniu z aplikacjami wykorzystującymi tylko procesor, prawie bez narzutu programisty.\n",
        "\n",
        "Aby przydzielić i zwolnić pamięć oraz uzyskać wskaźnik, do którego można się odwoływać zarówno w kodzie hosta, jak i urządzenia, należy zastąpić wywołania `malloc` i `free` słowami `cudaMallocManaged` i `cudaFree`, jak w poniższym przykładzie:\n",
        "\n",
        "```cpp\n",
        "// CPU-only\n",
        "\n",
        "int N = 2<<20;\n",
        "size_t size = N * sizeof(int);\n",
        "\n",
        "int *a;\n",
        "a = (int *)malloc(size);\n",
        "\n",
        "// Use `a` in CPU-only program.\n",
        "\n",
        "free(a);\n",
        "```\n",
        "\n",
        "```cpp\n",
        "// Accelerated\n",
        "\n",
        "int N = 2<<20;\n",
        "size_t size = N * sizeof(int);\n",
        "\n",
        "int *a;\n",
        "// Note the address of `a` is passed as first argument.\n",
        "cudaMallocManaged(&a, size);\n",
        "\n",
        "// Use `a` on the CPU and/or on any GPU in the accelerated system.\n",
        "\n",
        "cudaFree(a);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6h4iSUR8R7D"
      },
      "source": [
        "---\n",
        "### Ćwiczenie: dostęp do tablicy zarówno z poziomu hosta jak i urządzenia\n",
        "\n",
        "Program `01-double-elements.cu` alokuje tablicę, inicjuje ją wartościami całkowitymi na hoście, próbuje podwoić każdą z  wartości równolegle na GPU, a następnie potwierdza, czy operacje podwajania powiodły się, na hoście. Obecnie program nie będzie działał: próbuje współdziałać zarówno na hoście, jak i na urządzeniu z tablicą ze wskaźnikiem `a`, ale przydzielił tylko tablicę (za pomocą `malloc`), aby była dostępna na hoście. Wskazówki:\n",
        "\n",
        "- `a` powinno być dostępne zarówno dla kodu hosta, jak i urządzenia.\n",
        "- Pamięć w `a` powinna być poprawnie zwolniona."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAMfaN3X8R7D"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_70 -o double-elements 01-double-elements.cu -run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhqvmQr58R7D"
      },
      "source": [
        "## Grid Size Work Amount Mismatch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlNiVPNj8R7E"
      },
      "source": [
        "---\n",
        "## Obsługa niezgodności konfiguracji bloku z liczbą potrzebnych wątków\n",
        "\n",
        "Może się zdarzyć, że nie można zapisać konfiguracji wykonania, która utworzy dokładną liczbę wątków potrzebnych do zrównoleglenia pętli.\n",
        "\n",
        "Typowy przykład dotyczy chęci wyboru optymalnych rozmiarów bloków. Na przykład, ze względu na cechy sprzętowe procesora graficznego, bloki zawierające wiele wątków będących wielokrotnością 32 są często pożądane w celu zwiększenia wydajności. Zakładając, że chcielibyśmy uruchomić bloki, z których każdy zawierał 256 wątków (wielokrotność 32) i musieliśmy uruchomić 1000 równoległych zadań (trywialnie mała liczba dla ułatwienia wyjaśnienia), to nie ma takiej liczby bloków, która dawałaby dokładną sumę 1000 wątków w siatce, ponieważ nie ma wartości całkowitej, która pomnożona przez 32 da równe 1000.\n",
        "\n",
        "Ten scenariusz można łatwo rozwiązać w następujący sposób:\n",
        "\n",
        "- Napisz konfigurację wykonania, która tworzy **więcej** wątków niż jest to konieczne do wykonania przydzielonej pracy.\n",
        "- Przekaż wartość jako argument do kernela (`N`), która reprezentuje całkowity rozmiar zestawu danych do przetworzenia lub całkowitą liczbę wątków potrzebnych do ukończenia pracy.\n",
        "- Po obliczeniu indeksu wątku w gridzie (za pomocą `tid+bid*bdim`), sprawdź, czy ten indeks nie przekracza `N`, a jeśli tak nie jest, wykonaj odpowiednią pracę kernela.\n",
        "\n",
        "Oto przykład sposobu pisania konfiguracji wykonania, gdy znane są zarówno `N`, jak i liczba wątków w bloku i nie można zagwarantować dokładnego dopasowania między liczbą wątków w gridzie a `N`. Zapewnia, że zawsze jest co najmniej tyle wątków, ile potrzeba dla `N` i maksymalnie tylko 1 dodatkowy blok dodatkowych wątków:\n",
        "\n",
        "```cpp\n",
        "// Assume `N` is known\n",
        "int N = 100000;\n",
        "\n",
        "// Assume we have a desire to set `threads_per_block` exactly to `256`\n",
        "size_t threads_per_block = 256;\n",
        "\n",
        "// Ensure there are at least `N` threads in the grid, but only 1 block's worth extra\n",
        "size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "some_kernel<<<number_of_blocks, threads_per_block>>>(N);\n",
        "```\n",
        "\n",
        "Ponieważ powyższa konfiguracja wykonania skutkuje większą liczbą wątków w gridzie niż `N`, należy zachować ostrożność wewnątrz definicji `some_kernel`, aby `some_kernel` nie próbowało uzyskać dostępu do elementów danych spoza zakresu podczas wykonywania przez jeden z \"dodatkowych\" wątków:\n",
        "\n",
        "```cpp\n",
        "__global__ some_kernel(int N)\n",
        "{\n",
        "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "  if (idx < N) // Check to make sure `idx` maps to some value within `N`\n",
        "  {\n",
        "    // Only do work if it does\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4pXDcre8R7E"
      },
      "source": [
        "---\n",
        "### Ćwiczenie: Przyspieszenie pętli For z niedopasowaną konfiguracją wykonywania\n",
        "\n",
        "Program w `02-mismatched-config-loop.cu` alokuje pamięć, używając `cudaMallocManaged` dla 1000-elementowej tablicy liczb całkowitych, a następnie próbuje zainicjować wszystkie wartości tablicy równolegle przy użyciu kernela CUDA. Ten program zakłada, że znane są zarówno `N` jak i liczba `threads_per_block`. Twoim zadaniem jest:\n",
        "\n",
        "- Przypisanie wartości do `number_of_blocks`, która upewni się, że jest co najmniej tyle wątków, ile jest elementów w `a` do pracy.\n",
        "- Zaktualizowanie kernela `initializeElementsTo`, aby upewnić się, że nie próbuje wykonywać operacji na elementach danych, które są poza zakresem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgmZAj4j8R7E"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_70 -o mismatched-config-loop 02-mismatched-config-loop.cu -run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hygRi43V8R7E"
      },
      "source": [
        "---\n",
        "## Grid-Stride Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiTkNANe8R7E"
      },
      "source": [
        "---\n",
        "## Zbiór danych większy niż grid\n",
        "\n",
        "Albo z wyboru, często w celu stworzenia najbardziej wydajnej konfiguracji wykonania, albo z konieczności, liczba wątków w gridzie może być mniejsza niż rozmiar zbioru danych. Rozważ tablicę z 1000 elementów i grid z 250 wątkami (używając tutaj trywialnych rozmiarów dla ułatwienia wyjaśnienia). Tutaj każdy wątek w gridzie będzie musiał zostać użyty 4 razy. Jedną z popularnych metod, aby to zrobić, jest użycie **pętli grid-stride** w kernelu.\n",
        "\n",
        "W pętli grid-stride każdy wątek obliczy swój unikalny indeks w gridzie za pomocą `tid+bid*bdim`, wykona operację na elemencie o tym indeksie w tablicy, a następnie doda do swojego indeksu liczbę wątków w gridzie i powtarza operacje, aż znajdzie się poza zasięgiem tablicy. Na przykład dla tablicy 500 elementów i grid 250 wątków, wątek z indeksem 20 w gridzie:\n",
        "\n",
        "- Wykonaj operację na elemencie 20 z 500 elementów tablicy\n",
        "- Zwiększ jego indeks o 250, rozmiar grid, co daje 270\n",
        "- Wykonaj operację na elemencie 270 z 500 elementów tablicy\n",
        "- Zwiększ swój indeks o 250, rozmiar grid, co daje 520\n",
        "- Ponieważ 520 jest teraz poza zasięgiem tablicy, wątek przestanie działać\n",
        "\n",
        "CUDA dostarcza specjalną zmienną podającą liczbę bloków w siatce, `gridDim.x`. Obliczenie całkowitej liczby wątków w siatce to po prostu liczba bloków w siatce pomnożona przez liczbę wątków w każdym bloku, `gridDim.x * blockDim.x`. Mając to na uwadze, oto szczegółowy przykład pętli grid-stride w kernelu:\n",
        "\n",
        "```cpp\n",
        "__global__ void kernel(int *a, int N)\n",
        "{\n",
        "  int indexWithinTheGrid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int gridStride = gridDim.x * blockDim.x;\n",
        "\n",
        "  for (int i = indexWithinTheGrid; i < N; i += gridStride)\n",
        "  {\n",
        "    // do work on a[i];\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N_D5lJv8R7E"
      },
      "source": [
        "---\n",
        "### Ćwiczenie: wykorzystywanie pętli grid-stride\n",
        "\n",
        "Refaktoryzuj `03-grid-stride-double.cu`, aby użyć pętli grid-stride w kernelu `doubleElements` w kolejności że grid, który jest mniejszy niż `N`, może ponownie wykorzystać wątki, aby pokryć każdy element tablicy. Program wypisze czy każdy element w tablicy został podwojony, obecnie program dokładnie wypisuje `FALSE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHYhVkE-8R7E"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_70 -o grid-stride-double 03-grid-stride-double.cu -run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwwhxRHZ8R7L"
      },
      "source": [
        "---\n",
        "## Obsługa błędów\n",
        "\n",
        "Jak w każdej aplikacji, obsługa błędów w przyspieszonym kodzie CUDA jest niezbędna. Wiele, jeśli nie większość funkcji CUDA (patrz na przykład [funkcje zarządzania pamięcią](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY)) zwraca wartość typu `cudaError_t`, którego można użyć do sprawdzenia, czy wystąpił błąd podczas wywoływania funkcji. Oto przykład, w którym obsługa błędów jest wykonywana dla wywołania `cudaMallocManaged`:\n",
        "\n",
        "```cpp\n",
        "cudaError_t err;\n",
        "err = cudaMallocManaged(&a, N)                    // Assume the existence of `a` and `N`.\n",
        "\n",
        "if (err != cudaSuccess)                           // `cudaSuccess` is provided by CUDA.\n",
        "{\n",
        "  printf(\"Error: %s\\n\", cudaGetErrorString(err)); // `cudaGetErrorString` is provided by CUDA.\n",
        "}\n",
        "```\n",
        "\n",
        "Uruchamiane kerneli, które są zdefiniowane jako zwracające `void`, nie zwracają wartości typu `cudaError_t`. Aby sprawdzić błędy występujące podczas uruchamiania kernela, na przykład jeśli konfiguracja uruchamiania jest błędna, CUDA udostępnia funkcję `cudaGetLastError`, która zwraca wartość typu `cudaError_t`.\n",
        "\n",
        "```cpp\n",
        "/*\n",
        " * This launch should cause an error, but the kernel itself\n",
        " * cannot return it.\n",
        " */\n",
        "\n",
        "someKernel<<<1, -1>>>();  // -1 is not a valid number of threads.\n",
        "\n",
        "cudaError_t err;\n",
        "err = cudaGetLastError(); // `cudaGetLastError` will return the error from above.\n",
        "if (err != cudaSuccess)\n",
        "{\n",
        "  printf(\"Error: %s\\n\", cudaGetErrorString(err));\n",
        "}\n",
        "```\n",
        "\n",
        "Aby wychwycić błędy, które występują asynchronicznie, na przykład podczas wykonywania asynchronicznego kernela, konieczne jest sprawdzenie stanu zwróconego przez kolejne synchronizujące wywołanie API środowiska wykonawczego CUDA, takie jak `cudaDeviceSynchronize`, które zwróci błąd, jeśli jedno z uruchomionych wcześniej kerneli powinno zawieść.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQbtgxE_8R7L"
      },
      "source": [
        "---\n",
        "### Ćwiczenie: dodawanie obsługi błędów\n",
        "\n",
        "Obecnie `01-add-error-handling.cu` kompiluje, uruchamia i wyświetla, że elementy tablicy nie zostały pomyślnie podwojone. Program nie wskazuje jednak, że są w nim jakieś błędy. Refaktoryzuj aplikację do obsługi błędów CUDA, dzięki czemu możesz dowiedzieć się, co jest nie tak z programem i skutecznie go debugować. Będziesz musiał zbadać zarówno błędy synchroniczne potencjalnie tworzone podczas wywoływania funkcji CUDA, jak i błędy asynchroniczne potencjalnie tworzone podczas wykonywania jądra CUDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLq5DCT08R7L"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_70 -o add-error-handling 01-add-error-handling.cu -run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPbGigfH8R7L"
      },
      "source": [
        "---\n",
        "### CUDA Error Handling Function\n",
        "\n",
        "Pomocne może być utworzenie makra, które otacza wywołania funkcji CUDA w celu sprawdzenia błędów. Oto przykład, możesz go użyć w pozostałych ćwiczeniach:\n",
        "\n",
        "```cpp\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "inline cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "  return result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "/*\n",
        " * The macro can be wrapped around any function returning\n",
        " * a value of type `cudaError_t`.\n",
        " */\n",
        "\n",
        "  checkCuda( cudaDeviceSynchronize() )\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiBvv5ua8R7L"
      },
      "source": [
        "---\n",
        "## Podsumowanie\n",
        "\n",
        "Po zajęciach powinniście być w stanie:\n",
        "\n",
        "- Pisać, kompilować i uruchamiać programy w języku C/C++, które zarówno wywołują funkcje procesora, jak i **uruchamiają** **kernele** GPU.\n",
        "- Kontrolować równoległą **hierarchię wątków** za pomocą **konfiguracji wykonywania**.\n",
        "- Refaktoryzować pętle szeregowe, aby wykonywać ich iteracje równolegle na GPU.\n",
        "- Przydzielać i zwalniać pamięć dostępną zarówno dla procesorów, jak i kart graficznych.\n",
        "- Obsługiwać błędy generowane przez kod CUDA.\n",
        "\n",
        "Pozwoli to na wykonanie ostatniego zadania:\n",
        "\n",
        "- Przyspiesz aplikacje wykorzystujące tylko procesor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHfadTdf8R7L"
      },
      "source": [
        "---\n",
        "### Ćwiczenie końcowe: Przyspiesz aplikację dodawania wektorów\n",
        "\n",
        "Poniższe zadanie wymaga wykorzystania wszystkiego, czego nauczyliście się do tej pory. Polega ono na przyspieszeniu programu dodawania wektorów tylko przez procesor, który choć nie jest najbardziej wyrafinowanym programem, daje możliwość skupienia się na tym, czego nauczyliście się o przyspieszaniu aplikacji przez GPU za pomocą CUDA.\n",
        "\n",
        "`01-vector-add.cu` zawiera działającą aplikację dodawania wektorów tylko dla procesora CPU. Przyspiesz jego funkcję `addVectorsInto`, aby działała jako kernel CUDA na GPU i wykonywała swoją pracę równolegle. **Rozwiązanie należy przesłać na michal.zimon@wat.edu.pl**. Wskazówki:\n",
        "\n",
        "- Rozszerz definicję `addVectorsInto` tak, aby był to kernel CUDA.\n",
        "- Wybierz i wykorzystaj działającą konfigurację wykonania, aby `addVectorsInto` uruchamiał się jako kernel CUDA.\n",
        "- Aktualizuj alokacje pamięci i zwalnianie pamięci, aby odzwierciedlić, że 3 wektory `a`, `b` i `result` muszą być dostępne przez kod hosta i urządzenia.\n",
        "- Refaktoryzacja `addVectorsInto`: zostanie uruchomiony wewnątrz pojedynczego wątku i wystarczy wykonać tylko jeden wątek na wektorach wejściowych. Upewnij się, że wątek nigdy nie będzie próbował uzyskać dostępu do elementów spoza zakresu wektorów wejściowych i zwróć uwagę, czy wątek musi pracować na więcej niż jednym elemencie wektorów wejściowych.\n",
        "- Dodaj obsługę błędów w lokalizacjach, w których kod CUDA może nie działać.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjpOENDF8R7L"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_70 -o vector-add 01-vector-add.cu -run"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}