{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"16rNgtQSbnRQ3Ftq5Tv_pafdmTDtucVgv","timestamp":1732790734868}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Reinforcement learning algorithms primer"],"metadata":{"id":"bfsXJ3OJfmzL"}},{"cell_type":"markdown","source":["## Environment model (grid world)"],"metadata":{"id":"BfYH-aXmfbRm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eM0QIzT8Hk7_"},"outputs":[],"source":["import numpy as np\n","\n","BOARD_ROWS = 4\n","BOARD_COLS = 4\n","WIN_STATEs = [(0, 0), (3, 3)]\n","LOSE_STATEs = []\n","START = (2, 0)\n","DETERMINISTIC = True\n","\n","\n","class Environment:\n","    def __init__(self, state=START):\n","\n","        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n","        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n","        self.board[1, 1] = -1\n","        self.state = state\n","        self.isEnd = False\n","        self.determine = DETERMINISTIC\n","\n","        self.strategy = np.zeros([BOARD_ROWS, BOARD_COLS, len(self.actions)]) + 0.25\n","\n","        self.state_values = {}\n","        self.reset_state_values_to_unknown()\n","\n","    def get_random_nonterminal_state(self):\n","        terminal_state = True\n","        state = None\n","        while terminal_state:\n","            state = (np.random.randint(BOARD_ROWS), np.random.randint(BOARD_COLS))\n","            terminal_state = self.isTerminalState(state)\n","        return state\n","\n","    def zeros_state_values(self):\n","        for i in range(BOARD_ROWS):\n","            for j in range(BOARD_COLS):\n","                self.state_values[(i, j)] = 0\n","\n","    def reset_state_values_to_unknown(self):\n","        for i in range(BOARD_ROWS):\n","            for j in range(BOARD_COLS):\n","                if self.isTerminalState((i, j)):\n","                    self.state_values[(i, j)] = 0\n","                else:\n","                    self.state_values[(i, j)] = -100000\n","\n","    def giveReward(self):\n","        if self.state in (WIN_STATEs):\n","            return 0\n","        elif self.state in (LOSE_STATEs):\n","            return 0\n","        else:\n","            return -1\n","\n","    def getReward(self, state, action):\n","        if self.isTerminalState(state):\n","            return 0\n","        else:\n","            return -1\n","\n","        # next = self.getNextState(state, action)\n","        # if next == WIN_STATE:\n","        #    return 0\n","        # elif next == LOSE_STATE:\n","        #    return 0\n","        # else:\n","        #    return -1\n","\n","    def getNextState(self, from_state, action):\n","\n","        if self.isTerminalState(from_state):\n","            return from_state\n","\n","        if action == \"up\":\n","            nxtState = (from_state[0] - 1, from_state[1])\n","        elif action == \"down\":\n","            nxtState = (from_state[0] + 1, from_state[1])\n","        elif action == \"left\":\n","            nxtState = (from_state[0], from_state[1] - 1)\n","        else:\n","            nxtState = (from_state[0], from_state[1] + 1)\n","        # if next state legal\n","        if (nxtState[0] >= 0) and (nxtState[0] <= (BOARD_ROWS - 1)):\n","            if (nxtState[1] >= 0) and (nxtState[1] <= (BOARD_COLS - 1)):\n","                # if nxtState != (1, 1):\n","                return nxtState\n","\n","        return from_state\n","\n","    def isTerminalState(self, state):\n","        if (state in (WIN_STATEs)) or (state in (LOSE_STATEs)):\n","            return True\n","        return False\n","\n","    def nxtPosition(self, action):\n","        \"\"\"\n","        action: up, down, left, right\n","        -------------\n","        0 | 1 | 2| 3|\n","        1 |\n","        2 |\n","        return next position\n","        \"\"\"\n","        if self.determine:\n","            if action == \"up\":\n","                nxtState = (self.state[0] - 1, self.state[1])\n","            elif action == \"down\":\n","                nxtState = (self.state[0] + 1, self.state[1])\n","            elif action == \"left\":\n","                nxtState = (self.state[0], self.state[1] - 1)\n","            else:\n","                nxtState = (self.state[0], self.state[1] + 1)\n","\n","        else:\n","            # non-deterministic\n","            action = self._chooseActionProb(action)\n","            nxtState = self.nxtPosition(action)\n","\n","        # if next state legal\n","        if (nxtState[0] >= 0) and (nxtState[0] <= (BOARD_ROWS - 1)):\n","            if (nxtState[1] >= 0) and (nxtState[1] <= (BOARD_COLS - 1)):\n","                # if nxtState != (1, 1):\n","                return nxtState\n","\n","        return (self.state)\n","\n","    def nextStateReward(self, current_state, action):\n","        \"\"\"\n","        action: up, down, left, right\n","        -------------\n","        0 | 1 | 2| 3|\n","        1 |\n","        2 |\n","        return next position\n","        \"\"\"\n","\n","        reward = self.getReward(current_state, action)\n","        nxtState = self.getNextState(current_state, action)\n","\n","        return [nxtState, reward]\n","\n","    def _chooseActionProb(self, action):\n","        if action == \"up\":\n","            return np.random.choice([\"up\", \"left\", \"right\"], p=[0.8, 0.1, 0.1])\n","        if action == \"down\":\n","            return np.random.choice([\"down\", \"left\", \"right\"], p=[0.8, 0.1, 0.1])\n","        if action == \"left\":\n","            return np.random.choice([\"left\", \"up\", \"down\"], p=[0.8, 0.1, 0.1])\n","        if action == \"right\":\n","            return np.random.choice([\"right\", \"up\", \"down\"], p=[0.8, 0.1, 0.1])\n","\n","    def showStateValues(self):\n","        print('---------STATE VALUES------------')\n","        for i in range(0, BOARD_ROWS):\n","            print('----------------------------------')\n","            out = '| '\n","            for j in range(0, BOARD_COLS):\n","                if True:\n","                    # \"{0:0.2f}\".format(self.state_values[(i, j)])\n","                    out += \"{0:0.2f}\".format(self.state_values[(i, j)]).ljust(6) + ' | '\n","            print(out)\n","        print('----------------------------------')\n","\n","    def showStrategy(self):\n","        print('---------STRATEGY------------')\n","        for i in range(0, BOARD_ROWS):\n","            print('----------------------------------------------------------------------')\n","            out = '| '\n","            for j in range(0, BOARD_COLS):\n","                if True:\n","                    out += str(self.strategy[i, j]).ljust(6) + ' | '\n","            print(out)\n","        print('---------------------------------------------------------------------------')\n","\n","    def random_policy(self, state):\n","        return [1.0 / len(self.actions) for action in self.actions]\n","\n","    def upright_policy(self, state):\n","        return [0.49, 0.01, 0.01, 0.49]\n","\n","    def apply_strategy(self, state):\n","        return self.strategy[state[0], state[1]]\n","\n","    def DP(self, gamma=1):\n","        state_value_improvement = True\n","        k = 0\n","\n","        while state_value_improvement:\n","\n","            state_value_improvement = False\n","            new_state_values = {}\n","\n","            for state, v_old in self.state_values.items():\n","                if self.isTerminalState(state) == True:\n","                    new_state_values[state] = self.state_values[state]\n","                if self.isTerminalState(state) == False:\n","                    v_new = v_old\n","                    for j in range(len(self.actions)):\n","                        a = self.actions[j]\n","                        next_state, reward = self.nextStateReward(state, a)\n","                        next_state_transition_value = reward + self.state_values[next_state]\n","                        if v_new < next_state_transition_value:\n","                            v_new = next_state_transition_value\n","                    new_state_values[state] = v_new\n","                    if abs(v_old - v_new) > 0:\n","                        state_value_improvement = True\n","            self.state_values = new_state_values\n","            k = k + 1\n","\n","    def value_iteration(self, epsilon, gamma=1):\n","        '''\n","\n","        :param policy:\n","        :param epsilon:\n","        :param gamma:\n","        :return:\n","        '''\n","\n","        k = 0\n","        state_value_improvement = True\n","        while state_value_improvement:\n","            state_value_improvement = False\n","            k = k + 1\n","\n","            for state, v_old in self.state_values.items():\n","                if not self.isTerminalState(state):\n","                    action_reward = 0\n","                    for j in range(len(self.actions)):\n","                        action = self.actions[j]\n","                        prob = self.strategy[state[0], state[1], j]\n","                        next_state, reward = self.nextStateReward(state, action)\n","                        action_reward = action_reward + prob * (gamma * self.state_values[next_state] + reward)\n","\n","                    if abs(self.state_values[state] - action_reward) > epsilon:\n","                        state_value_improvement = True\n","\n","                    self.state_values[state] = action_reward\n","        print(\"Iterations\", k)\n","\n","    def greedily_update_policy(self, epsilon, gamma=1):\n","        for state, v_old in self.state_values.items():\n","            self.strategy[state[0], state[1]] = np.zeros(4)\n","\n","            if not self.isTerminalState(state):\n","                action_reward = np.zeros(4)\n","                for j in range(len(self.actions)):\n","                    action = self.actions[j]\n","                    next_state, reward = self.nextStateReward(state, action)\n","                    action_reward[j] = gamma * self.state_values[next_state] + reward\n","\n","                best_action = np.argmax(action_reward)\n","                self.strategy[state[0], state[1], best_action] = 1.\n"]},{"cell_type":"markdown","source":["## Model free agent for Q-Learning"],"metadata":{"id":"uqBV4zfefglq"}},{"cell_type":"code","source":["class Agent:\n","\n","    def __init__(self, zero_initial_values=False):\n","        # self.states_visited_trace = []  # record position and action taken at the position\n","        # self.states_reward_trace = []  # record position and action taken at the position\n","\n","        # self.full_states_log=[]\n","        # self.actions = [\"up\", \"down\", \"left\", \"right\"]\n","        self.environment = Environment()\n","        # self.isEnd = False\n","        self.lr = 0.01\n","        self.exp_rate = 0.3\n","        self.decay_gamma = 1.0\n","\n","        # initial state reward\n","        self.state_values = {}\n","        self.state_visit_count = {}\n","        self.current_state = self.environment.get_random_nonterminal_state()\n","\n","        self.moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n","\n","        if zero_initial_values:\n","            self.Q = np.zeros((BOARD_ROWS, BOARD_COLS, len(self.moves)))\n","        else:\n","            self.Q = np.random.uniform(0, 0.5, size=(BOARD_ROWS, BOARD_COLS, len(self.moves)))\n","\n","    def chooseActionAccordingToPolicy(self, policy):\n","        # choose action with most expected value\n","        probs = policy\n","        cumulative_probs = np.cumsum(probs)\n","        action_index = np.digitize(np.random.uniform(0, 1), cumulative_probs)\n","        return self.environment.actions[action_index]\n","\n","    def reset(self):\n","        self.states_visited_trace = []\n","        self.states_reward_trace = []\n","        self.environment = Environment()\n","        self.current_state = self.environment.get_random_nonterminal_state()\n","\n","    def generate_episode(self):\n","        episode = []\n","        self.reset()\n","\n","        while self.environment.isTerminalState(self.current_state) == False:\n","            state_from = self.current_state\n","            action = self.chooseActionAccordingToPolicy(self.environment.random_policy(state_from))\n","            target_state, reward = self.environment.nextStateReward(state_from, action)\n","            episode.append((state_from, action, reward))\n","            self.current_state = target_state\n","        return episode\n","\n","    def q_learning_choose_action(self, episode, number_of_episodes, state):\n","        epsilon = 1 - episode / number_of_episodes\n","\n","        if np.random.uniform(0, 1) < epsilon:\n","            moves_to_choose = []\n","            for move_ind, move in enumerate(self.moves):\n","                is_good_row = state[0] + move[0] in range(BOARD_ROWS)\n","                is_good_col = state[1] + move[1] in range(BOARD_COLS)\n","                if is_good_row and is_good_col:\n","                    moves_to_choose.append(move_ind)\n","            action = np.random.choice(moves_to_choose)\n","        else:\n","            action = np.argmax(self.Q[state])\n","\n","        return action\n","\n","    def q_learning_execute_action(self, state, action):\n","\n","        if self.environment.isTerminalState(state):\n","            raise ValueError(\"Terminal current state: \" + str(state))\n","\n","        new_state = (\n","            max(0, min(BOARD_ROWS - 1, state[0] + self.moves[action][0])),\n","            max(0, min(BOARD_COLS - 1, state[1] + self.moves[action][1]))\n","        )\n","\n","        if new_state[0] not in range(BOARD_ROWS) and new_state[1] not in range(BOARD_COLS):\n","            raise ValueError(\"Bad new state: \" + str(new_state))\n","\n","        return new_state\n","\n","    def q_learning(self, number_of_episodes: int = 100, lr: float = 0.1, gamma: float = 0.99):\n","\n","        for episode in range(number_of_episodes):\n","\n","            current_state = self.current_state\n","\n","            while not self.environment.isTerminalState(current_state):\n","                action = self.q_learning_choose_action(episode, number_of_episodes, current_state)\n","                next_state = self.q_learning_execute_action(current_state, action)\n","                _, reward = self.environment.nextStateReward(next_state, action)\n","\n","                # print(f\"Episode: {episode}, State: {current_state}, Action: {self.moves[action]}, Next State: {next_state}, Reward: {reward}\")\n","\n","                # Równanie Bellmana\n","                # print(f\"Q[current_state][action]: {self.Q[current_state][action]}, np.max(self.Q[next_state]): {np.max(self.Q[next_state])}, self.Q[current_state][action]: {self.Q[current_state][action]}\")\n","                self.Q[current_state][action] += lr * (\n","                        reward\n","                        + gamma * np.max(self.Q[next_state])\n","                        - self.Q[current_state][action]\n","                )\n","\n","                current_state = next_state\n","\n","    def q_learning_show_values(self):\n","        print(f\"STATE VALUES\".center(117, '-'))\n","        for i in range(0, BOARD_ROWS):\n","            print(\"-\" * 117)\n","            out = '| '\n","            for j in range(0, BOARD_COLS):\n","                values_set = self.Q[i][j]\n","                values_str = ', '.join([\"{0:0.2f}\".format(value).ljust(5) for value in values_set])\n","                out += \"{}\".format(values_str).ljust(6) + ' | '\n","            print(out)\n","        print(\"-\" * 117)\n"],"metadata":{"id":"UBhUhymLfYvX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q Learning"],"metadata":{"id":"Db1q4_BUlr_l"}},{"cell_type":"code","source":["def q_learning_zeroes():\n","    print(f\"Q-Learning with initial values 0\".center(117, '-'))\n","    agent = Agent(zero_initial_values=True)\n","    agent.q_learning()\n","    agent.q_learning_show_values()\n","    print(\"-\" * 117)\n","\n","\n","def q_learning_random():\n","    print(f\"Q-Learning with initial values random\".center(117, '-'))\n","    agent = Agent(zero_initial_values=False)\n","    agent.q_learning()\n","    agent.q_learning_show_values()\n","    print(\"-\" * 117)\n","\n","\n","if __name__ == '__main__':\n","    q_learning_zeroes()\n","    q_learning_random()\n"],"metadata":{"id":"IJrk8SBylqIb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732802588238,"user_tz":-60,"elapsed":267,"user":{"displayName":"Radek Relidzyński","userId":"03549375523222306421"}},"outputId":"41a58abd-401b-4617-c01c-2e10716443c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------Q-Learning with initial values 0------------------------------------------\n","-----------------------------------------------------STATE VALUES----------------------------------------------------\n","---------------------------------------------------------------------------------------------------------------------\n","| 0.00 , 0.00 , 0.00 , 0.00  | -0.10, -0.20, 0.00 , 0.00  | -0.20, -0.21, -0.27, -0.20 | -0.30, -0.30, -0.30, -0.20 | \n","---------------------------------------------------------------------------------------------------------------------\n","| 0.00 , -0.28, 0.00 , -0.62 | -0.47, -0.45, -0.41, -0.42 | -0.34, -0.63, -0.40, -0.41 | -0.37, -0.34, -0.35, -0.30 | \n","---------------------------------------------------------------------------------------------------------------------\n","| -0.65, -0.52, -0.20, -0.25 | -0.63, -0.68, -0.56, -0.61 | -0.77, -0.75, -0.79, -0.75 | -0.34, 0.00 , -0.57, 0.00  | \n","---------------------------------------------------------------------------------------------------------------------\n","| -0.63, -0.60, -0.50, -1.31 | -0.90, -1.00, -0.96, -0.91 | -1.47, -0.10, -1.42, 0.00  | 0.00 , 0.00 , 0.00 , 0.00  | \n","---------------------------------------------------------------------------------------------------------------------\n","---------------------------------------------------------------------------------------------------------------------\n","----------------------------------------Q-Learning with initial values random----------------------------------------\n","-----------------------------------------------------STATE VALUES----------------------------------------------------\n","---------------------------------------------------------------------------------------------------------------------\n","| 0.34 , 0.24 , 0.17 , 0.12  | 0.15 , -0.67, 0.33 , -1.28 | -0.77, -1.58, -0.68, -1.59 | -1.71, -1.68, -1.67, -1.77 | \n","---------------------------------------------------------------------------------------------------------------------\n","| 0.19 , -0.07, 0.11 , -0.36 | -0.46, -0.53, -0.49, -0.60 | -1.02, -0.98, -0.97, -1.18 | -1.51, -0.70, -1.30, -0.77 | \n","---------------------------------------------------------------------------------------------------------------------\n","| -0.14, 0.18 , 0.16 , -0.41 | -0.49, -0.26, -0.21, -0.40 | -0.74, -0.41, -0.48, -0.43 | -1.21, 0.31 , -0.74, 0.11  | \n","---------------------------------------------------------------------------------------------------------------------\n","| -0.02, 0.27 , 0.31 , 0.04  | -0.15, 0.02 , -0.11, -0.31 | -0.58, 0.39 , -0.29, 0.39  | 0.32 , 0.16 , 0.11 , 0.30  | \n","---------------------------------------------------------------------------------------------------------------------\n","---------------------------------------------------------------------------------------------------------------------\n"]}]}]}